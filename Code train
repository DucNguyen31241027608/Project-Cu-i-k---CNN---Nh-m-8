# ======================================
# üß† TRAINING CNN MODEL - N√ÇNG C·∫§P CHUY√äN S√ÇU
# ======================================
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# --- 1. T·∫°o generator T√ÅCH BI·ªÜT augmentation cho t·∫≠p train ---
# Augmentation ch·ªâ √°p d·ª•ng cho t·∫≠p hu·∫•n luy·ªán
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    rotation_range=20, # TƒÉng nh·∫π rotation
    zoom_range=0.25,   # TƒÉng nh·∫π zoom
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest', # Th√™m fill_mode
    validation_split=0.2 # Keep validation_split here for the training generator
)

# Generator KH√îNG AUGMENTATION cho t·∫≠p validation/test - REMOVED THIS AS WE USE ONE GENERATOR
# val_test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
#     rescale=1./255, # Remove validation_split here
#     validation_split=0.2 # Add validation_split back for the validation generator
# )

train_gen = train_datagen.flow_from_directory(
    "/content/drive/MyDrive/ai/train",
    target_size=(128,128),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

# Use train_datagen for the validation generator as well, specifying subset='validation'
val_gen = train_datagen.flow_from_directory(
    "/content/drive/MyDrive/ai/train",
    target_size=(128,128),
    batch_size=32,
    class_mode='categorical',
    subset='validation' # Still use subset='validation' with the validation generator
)


# --- 2. Ki·∫øn tr√∫c CNN c·∫£i ti·∫øn (ƒêi·ªÅu ch·ªânh Filters v√† Kernel) ---
model = Sequential([
    # T·∫ßng 1: D√πng kernel 5x5 ƒë·ªÉ b·∫Øt ƒë·∫∑c tr∆∞ng l·ªõn, sau ƒë√≥ l√† 3x3
    Conv2D(32, (5,5), activation='relu', padding='same', input_shape=(128,128,3)),
    BatchNormalization(),
    MaxPooling2D(2,2),

    # T·∫ßng 2
    Conv2D(64, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    # T·∫ßng 3: TƒÉng filters l√™n 128
    Conv2D(128, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    # T·∫ßng 4: Th√™m 1 t·∫ßng Conv n·ªØa v·ªõi 256 filters (ho·∫∑c gi·ªØ 128)
    Conv2D(256, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(2,2),


    Flatten(),
    Dropout(0.5),
    # Gi·∫£m s·ªë n∆°-ron ·ªü t·∫ßng Dense ƒë·ªÉ gi·∫£m overfitting/tham s·ªë
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dense(train_gen.num_classes, activation='softmax')
])

# --- 3. Bi√™n d·ªãch model ---
# Gi·ªØ nguy√™n Adam 0.0005, l√† m·ªôt l·ª±a ch·ªçn t·ªët
model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# --- 4. Callback ch·ªëng overfitting & t·ªëi ∆∞u h·ªçc (ƒêi·ªÅu ch·ªânh th√¥ng s·ªë) ---
callbacks = [
    # Theo d√µi val_loss, ki√™n nh·∫´n h∆°n m·ªôt ch√∫t
    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
    # Theo d√µi val_accuracy, gi·∫£m LR 50%
    ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=1e-7, mode='max', verbose=1),
    ModelCheckpoint(
        "/content/drive/MyDrive/ai/model/best_cnn_model_v2.h5",
        monitor='val_accuracy',
        save_best_only=True,
        mode='max' # ƒê·∫£m b·∫£o theo d√µi val_accuracy theo mode 'max'
    )
]

# ==== Hu·∫•n luy·ªán model ====
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=100,
    callbacks=callbacks
)

# ==== L∆∞u model cu·ªëi c√πng ====
model.save("/content/drive/MyDrive/ai/model/final_cnn_model_v2.h5")

print("‚úÖ M√¥ h√¨nh CNN n√¢ng c·∫•p ƒë√£ ƒë∆∞·ª£c train v√† l∆∞u th√†nh c√¥ng!")
